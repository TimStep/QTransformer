GPT2 like LLM using quantum circuits for attention mechanism

We replace the key, query and value matrices with variational quantum circuits effectively reducing the number of trainable parameters
