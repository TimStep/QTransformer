{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcbC8tLc3bDgp97NLBo6uq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimStep/QTransformer/blob/main/QGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpEJcpcXcjYT"
      },
      "outputs": [],
      "source": [
        "!pip install -q pennylane transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pennylane as qml\n",
        "import torch\n",
        "#from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import sklearn.metrics as metrics\n",
        "from transformer_lens import HookedTransformer"
      ],
      "metadata": {
        "id": "F_H0je0UfVes"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 5678\n",
        "torch.manual_seed(seed=RANDOM_SEED)\n",
        "torch.cuda.manual_seed(seed=RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Ah-xV1q5f69l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FWeV7j1Ef-Sy",
        "outputId": "19ef8f54-df22-4f7b-80fc-42b1c9c09424"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "FqpZjp0eOLmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#not working lately, waiting until https://huggingface.co/openai-community/gpt2 comes back to life\n",
        "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
        "reference_text = [\"Today we are going to implement a Transformer from scratch!\", \"Today we are going to implement a Transformer from scratch!\"]\n",
        "text_tokens = reference_gpt2.to_tokens(reference_text).to(device)"
      ],
      "metadata": {
        "id": "b-jef8DSPRQD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "H416tK0Oh9Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    #classical params\n",
        "    d_model: int = 768 #embedding size\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024 #context length\n",
        "    n_heads: int = 12 #number of attention heads\n",
        "    n_layers: int = 12 #number of transformer blocks\n",
        "    dropout: float = 0.1\n",
        "    tying = False\n",
        "    #quantum params\n",
        "    query_depth: int = 1\n",
        "    key_depth: int = 2\n",
        "    value_depth: int = 3\n",
        "    q_device: str = \"lightning.qubit\"\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "3U4O5UAfh_H4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding, MLP and other stuff"
      ],
      "metadata": {
        "id": "9eATEdf0P6Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(torch.nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.wte = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        self.wpe = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
        "\n",
        "        nn.init.normal_(self.wte, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.wpe, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "\n",
        "        tok_emb = self.wte[tokens]\n",
        "        pos_emb = self.wpe[torch.arange(tokens.shape[1])]\n",
        "        embeddings = tok_emb + pos_emb\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "WD2zhUlUQeHu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.w_in = nn.Linear(cfg.d_model, 4 * cfg.d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.w_out = nn.Linear(4 * cfg.d_model, cfg.d_model)\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.w_in(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.w_out(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lEBAnB2QVVLE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembed(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg, tying=None): #tying should be the W_E matrix\n",
        "        super().__init__()\n",
        "\n",
        "        self.unembed = nn.Linear(cfg.d_model, cfg.d_vocab)\n",
        "        if tying: self.unembed.weight = tying\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.unembed(x)\n"
      ],
      "metadata": {
        "id": "SR_TS6R4XBzd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention and transformer block"
      ],
      "metadata": {
        "id": "lhG9998lgjL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.qkv_depth = (cfg.query_depth, cfg.key_depth, cfg.value_depth)\n",
        "        self.n_qubits = int(math.ceil(math.log2(cfg.d_model)))\n",
        "        self.device = cfg.q_device\n",
        "\n",
        "        # init device\n",
        "        self.dev = qml.device(self.device, shots=None, wires=self.n_qubits)\n",
        "\n",
        "        # init weights\n",
        "        #context size equals to the number of parallel quantum circuits we need\n",
        "        #self.embedding_circuit = torch.nn.Parameter(torch.empty(self.qkv_depth[0], self.n_qubits))\n",
        "        self.query_weights = nn.Parameter(torch.empty(self.qkv_depth[0], self.n_qubits))\n",
        "        self.key_weights = nn.Parameter(torch.empty(self.qkv_depth[1], self.n_qubits))\n",
        "        self.value_weights = nn.Parameter(torch.empty(self.qkv_depth[2], self.n_qubits))\n",
        "        #output projection is kept classical\n",
        "        self.W_O = nn.Linear(self.n_qubits, cfg.d_model)\n",
        "\n",
        "        self.reset_weights()\n",
        "\n",
        "        # init QNode\n",
        "        self.query_node = qml.QNode(self.queryCircuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
        "        self.key_node = qml.QNode(self.keyCircuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
        "        self.value_node = qml.QNode(self.valueCircuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
        "\n",
        "    def queryCircuit(self, inputs, weights, depth):\n",
        "\n",
        "        #quantum embedding\n",
        "        inputs = inputs.detach() #amplitude embediding does not support differentiable tensors\n",
        "        qml.AmplitudeEmbedding(inputs, range(self.n_qubits), normalize=True, pad_with=0)\n",
        "\n",
        "\n",
        "        #VQC\n",
        "        for j in range(depth):\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.RY(weights[j, i], wires=[i])\n",
        "\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.CNOT(wires=[i % self.n_qubits, (i + 1) % self.n_qubits])\n",
        "\n",
        "        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits)]\n",
        "\n",
        "    def keyCircuit(self, inputs, weights, depth):\n",
        "        return self.queryCircuit(inputs, weights, depth)\n",
        "\n",
        "    def valueCircuit(self, inputs, weights, depth):\n",
        "        return self.queryCircuit(inputs, weights, depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x = (B, seq_len, emb_len)\n",
        "        #flatten all batches into one sequence x = (B*seq_len, emb_len) for q_node\n",
        "        B, T = x.shape[:2] #save the batch size and sequence length for unflattening\n",
        "\n",
        "        x = torch.flatten(x, start_dim=0, end_dim=1)\n",
        "        q = self.query_node(x, self.query_weights, self.qkv_depth[0])\n",
        "        q = torch.stack(q, dim=-1) # q = (B*seq_len, n_qubit)\n",
        "        q = torch.unflatten(q, 0, (B, T)) #q = (B, seq_len, n_qubit)\n",
        "        q = torch.unsqueeze(q, 1) #torch attention expects size[1] to be number of heads, in our case it is always one\n",
        "\n",
        "        #same with key and value\n",
        "        k = self.key_node(x, self.key_weights, self.qkv_depth[1])\n",
        "        k = torch.stack(k, dim=-1)\n",
        "        k = torch.unflatten(k, 0, (B, T))\n",
        "        k = torch.unsqueeze(k, 1)\n",
        "\n",
        "        v = self.value_node(x, self.value_weights, self.qkv_depth[2])\n",
        "        v = torch.stack(v, dim=-1)\n",
        "        v = torch.unflatten(v, 0, (B, T))\n",
        "        v = torch.unsqueeze(v, 1)\n",
        "        #print(\"Query, Key, Value shapes:\")\n",
        "        #print(q.shape, k.shape, v.shape)\n",
        "\n",
        "        #attention\n",
        "        att = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        att = torch.squeeze(att) #remove the useless nhead dimension\n",
        "        x = torch.unflatten(x, 0, (B, T))\n",
        "        #print(\"Attention shape:\")\n",
        "        #print(att.shape)\n",
        "\n",
        "        x = self.W_O(att)\n",
        "        #print(\"Residual shape:\")\n",
        "        #print(x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        pass\n",
        "\n",
        "    def reset_weights(self):\n",
        "\n",
        "        nn.init.uniform_(self.query_weights, a=0, b=2 * torch.pi)\n",
        "        nn.init.uniform_(self.key_weights, a=0, b=2 * torch.pi)\n",
        "        nn.init.uniform_(self.value_weights, a=0, b=2 * torch.pi)\n",
        "\n",
        "    def draw_circuit(self):\n",
        "        sample_input = torch.randn((self.cfg.d_model,))\n",
        "\n",
        "        query_drawer = qml.draw(self.query_node)\n",
        "        query_diagram = query_drawer(sample_input, self.query_weights, self.qkv_depth[0])\n",
        "\n",
        "        key_drawer = qml.draw(self.key_node)\n",
        "        key_diagram = key_drawer(sample_input, self.key_weights, self.qkv_depth[1])\n",
        "\n",
        "        value_drawer = qml.draw(self.value_node)\n",
        "        value_diagram = value_drawer(sample_input, self.value_weights, self.qkv_depth[2])\n",
        "\n",
        "        '''\n",
        "        print(\"Query circuit:\")\n",
        "        print(query_diagram)\n",
        "        print(\"Key circuit:\")\n",
        "        print(key_diagram)\n",
        "        print(\"Value circuit:\")\n",
        "        print(value_diagram)\n",
        "        '''\n",
        "\n",
        "    def draw_circuit_mpl(self):\n",
        "        # Generate a sample input and weights for visualization\n",
        "        sample_input = torch.randn((self.n_qubits,))\n",
        "\n",
        "        # Use qml.draw_mpl to plot the circuit\n",
        "        qml.draw_mpl(self.query_node)(sample_input, self.weights[0])\n",
        "        plt.title(\"Quantum Circuit\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "D3-CU0x8gnI_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.ln1 = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "        self.ln2 = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "        self.attn = QAttention(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3_yfxgvtY9m-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Transformer"
      ],
      "metadata": {
        "id": "eADST_JpPDiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGPT(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "\n",
        "        residual = self.embed(tokens)\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "        logits = self.unembed(self.ln_final(residual))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "bs8rBEGwvsCj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_and_load(model, reference_model): #load mlp and ln weights and freeze them\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      #wte and wpe matrices\n",
        "      model.embed.wte.copy_(reference_model.state_dict()['embed.W_E'])\n",
        "      model.embed.wpe.copy_(reference_model.state_dict()['pos_embed.W_pos'])\n",
        "      model.embed.wte.requires_grad_(False)\n",
        "      model.embed.wpe.requires_grad_(False)\n",
        "\n",
        "      #unembedding matrix\n",
        "      model.unembed.unembed.weight.copy_(reference_model.state_dict()['unembed.W_U'].T) #for some strange reason pretrained W_U is transposed\n",
        "      #print(model.unembed.unembed.bias.shape, reference_model.state_dict()['unembed.b_U'].shape)\n",
        "      model.unembed.unembed.bias.copy_(reference_model.state_dict()['unembed.b_U'])\n",
        "      model.unembed.unembed.requires_grad_(False)\n",
        "\n",
        "      #final LayerNorm\n",
        "      model.ln_final.weight.copy_(reference_model.state_dict()['ln_final.w'])\n",
        "      model.ln_final.bias.copy_(reference_model.state_dict()['ln_final.b'])\n",
        "      model.ln_final.requires_grad_(False)\n",
        "\n",
        "      for (n, block) in enumerate(model.blocks):\n",
        "\n",
        "        #LayerNorms\n",
        "        block.ln1.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln1.w'])\n",
        "        block.ln1.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln1.b'])\n",
        "        block.ln1.requires_grad_(False)\n",
        "        block.ln2.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln2.w'])\n",
        "        block.ln2.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln2.b'])\n",
        "        block.ln2.requires_grad_(False)\n",
        "        #MLP (the pretrained weights are transposed)\n",
        "        block.mlp.w_in.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.W_in'].T)\n",
        "        block.mlp.w_in.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.b_in'])\n",
        "        block.mlp.w_in.requires_grad_(False)\n",
        "        block.mlp.w_out.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.W_out'].T)\n",
        "        block.mlp.w_out.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.b_out'])\n",
        "        block.mlp.w_out.requires_grad_(False)\n"
      ],
      "metadata": {
        "id": "Y41-LsvP7nCY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt = QGPT(cfg)"
      ],
      "metadata": {
        "id": "TdMTjOwV3-XA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = sum(p.numel() for p in qgpt.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrtDfXfvqub5",
        "outputId": "b1b0cec8-e6b2-4f14-894c-6c165be5909e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 134841121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_and_load(qgpt, reference_gpt2)"
      ],
      "metadata": {
        "id": "CUg4ygYkebLL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = sum(p.numel() for p in qgpt.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCrbQ-fDqzmo",
        "outputId": "4e7efa1f-364c-4140-d04f-c8b97aa18467"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 102096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt(text_tokens).shape"
      ],
      "metadata": {
        "id": "aR2DR25Lo5qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d55a1394-59d5-4cbd-ab77-92f956d1ddda"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 13, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}