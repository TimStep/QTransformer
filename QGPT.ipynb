{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7RO7quogpLf7tRa9aEV3u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimStep/QTransformer/blob/main/QGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MpEJcpcXcjYT"
      },
      "outputs": [],
      "source": [
        "!pip install -q pennylane transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pennylane as qml\n",
        "import torch\n",
        "#from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import sklearn.metrics as metrics\n",
        "from transformer_lens import HookedTransformer"
      ],
      "metadata": {
        "id": "F_H0je0UfVes"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 5678\n",
        "torch.manual_seed(seed=RANDOM_SEED)\n",
        "torch.cuda.manual_seed(seed=RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Ah-xV1q5f69l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FWeV7j1Ef-Sy",
        "outputId": "f7e5a860-a8d1-4314-d616-8110df3a1a78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "FqpZjp0eOLmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
        "reference_text = [\"Jingle bells, jingle bells, jingle all the way\", \"Today I was walking home, when suddenly\"]\n",
        "text_tokens = reference_gpt2.to_tokens(reference_text).to(device)"
      ],
      "metadata": {
        "id": "b-jef8DSPRQD",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e03666-7749-4a4a-9bda-cd2008f828b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzHAmtBpfaah",
        "outputId": "d04abc1a-4e08-4625-9cdb-eb12b049d2fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 14])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = reference_gpt2.tokenizer\n",
        "tokenizer.decode(text_tokens[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "P_fOP_uKbloL",
        "outputId": "146255df-b979-4047-9f04-50daf6f5c334"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>Today I was walking home, when suddenly<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "H416tK0Oh9Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    #classical params\n",
        "    d_model: int = 768 #embedding size\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024 #context length\n",
        "    n_heads: int = 12 #number of attention heads\n",
        "    n_layers: int = 12 #number of transformer blocks\n",
        "    dropout: float = 0.1\n",
        "    tying = False\n",
        "    #quantum params\n",
        "    query_depth: int = 1\n",
        "    key_depth: int = 2\n",
        "    value_depth: int = 3\n",
        "    q_device: str = \"lightning.qubit\"\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "3U4O5UAfh_H4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding, MLP and other stuff"
      ],
      "metadata": {
        "id": "9eATEdf0P6Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(torch.nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.wte = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        self.wpe = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
        "\n",
        "        nn.init.normal_(self.wte, std=self.cfg.init_range)\n",
        "        nn.init.normal_(self.wpe, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "\n",
        "        tok_emb = self.wte[tokens]\n",
        "        pos_emb = self.wpe[torch.arange(tokens.shape[1])]\n",
        "        embeddings = tok_emb + pos_emb\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "WD2zhUlUQeHu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.w_in = nn.Linear(cfg.d_model, 4 * cfg.d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.w_out = nn.Linear(4 * cfg.d_model, cfg.d_model)\n",
        "        self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.w_in(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.w_out(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lEBAnB2QVVLE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembed(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg, tying=None): #tying should be the W_E matrix\n",
        "        super().__init__()\n",
        "\n",
        "        self.unembed = nn.Linear(cfg.d_model, cfg.d_vocab)\n",
        "        if tying: self.unembed.weight = tying\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.unembed(x)\n"
      ],
      "metadata": {
        "id": "SR_TS6R4XBzd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention and transformer block"
      ],
      "metadata": {
        "id": "lhG9998lgjL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.qkv_depth = (cfg.query_depth, cfg.key_depth, cfg.value_depth)\n",
        "        self.n_qubits = int(math.ceil(math.log2(cfg.d_model)))\n",
        "        self.device = cfg.q_device\n",
        "\n",
        "        # init device\n",
        "        self.dev = qml.device(self.device, shots=None, wires=self.n_qubits)\n",
        "\n",
        "        # init weights\n",
        "        #context size equals to the number of parallel quantum circuits we need\n",
        "        #self.embedding_circuit = torch.nn.Parameter(torch.empty(self.qkv_depth[0], self.n_qubits))\n",
        "        self.query_weights = nn.Parameter(torch.empty(self.qkv_depth[0], self.n_qubits))\n",
        "        self.key_weights = nn.Parameter(torch.empty(self.qkv_depth[1], self.n_qubits))\n",
        "        self.value_weights = nn.Parameter(torch.empty(self.qkv_depth[2], self.n_qubits))\n",
        "        #output projection is kept classical\n",
        "        self.W_O = nn.Linear(self.n_qubits, cfg.d_model)\n",
        "\n",
        "        self.reset_weights()\n",
        "\n",
        "        # init QNode\n",
        "        self.query_node = qml.QNode(self.queryCircuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
        "        self.key_node = qml.QNode(self.keyCircuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
        "        self.value_node = qml.QNode(self.valueCircuit, self.dev, interface=\"torch\", diff_method=\"best\")\n",
        "\n",
        "    def queryCircuit(self, inputs, weights, depth):\n",
        "\n",
        "        #quantum embedding\n",
        "        inputs = inputs.detach() #amplitude embediding does not support differentiable tensors\n",
        "        qml.AmplitudeEmbedding(inputs, range(self.n_qubits), normalize=True, pad_with=0)\n",
        "\n",
        "\n",
        "        #VQC\n",
        "        for j in range(depth):\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.RY(weights[j, i], wires=[i])\n",
        "\n",
        "            for i in range(self.n_qubits):\n",
        "                qml.CNOT(wires=[i % self.n_qubits, (i + 1) % self.n_qubits])\n",
        "\n",
        "        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits)]\n",
        "\n",
        "    def keyCircuit(self, inputs, weights, depth):\n",
        "        return self.queryCircuit(inputs, weights, depth)\n",
        "\n",
        "    def valueCircuit(self, inputs, weights, depth):\n",
        "        return self.queryCircuit(inputs, weights, depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x = (B, seq_len, emb_len)\n",
        "        #flatten all batches into one sequence x = (B*seq_len, emb_len) for q_node\n",
        "        B, T = x.shape[:2] #save the batch size and sequence length for unflattening\n",
        "\n",
        "        x = torch.flatten(x, start_dim=0, end_dim=1)\n",
        "        q = self.query_node(x, self.query_weights, self.qkv_depth[0])\n",
        "        q = torch.stack(q, dim=-1) # q = (B*seq_len, n_qubit)\n",
        "        q = torch.unflatten(q, 0, (B, T)) #q = (B, seq_len, n_qubit)\n",
        "        q = torch.unsqueeze(q, 1) #torch attention expects size[1] to be number of heads, in our case it is always one\n",
        "\n",
        "        #same with key and value\n",
        "        k = self.key_node(x, self.key_weights, self.qkv_depth[1])\n",
        "        k = torch.stack(k, dim=-1)\n",
        "        k = torch.unflatten(k, 0, (B, T))\n",
        "        k = torch.unsqueeze(k, 1)\n",
        "\n",
        "        v = self.value_node(x, self.value_weights, self.qkv_depth[2])\n",
        "        v = torch.stack(v, dim=-1)\n",
        "        v = torch.unflatten(v, 0, (B, T))\n",
        "        v = torch.unsqueeze(v, 1)\n",
        "        #print(\"Query, Key, Value shapes:\")\n",
        "        #print(q.shape, k.shape, v.shape)\n",
        "\n",
        "        #attention\n",
        "        att = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        att = torch.squeeze(att) #remove the useless nhead dimension\n",
        "        x = torch.unflatten(x, 0, (B, T))\n",
        "        #print(\"Attention shape:\")\n",
        "        #print(att.shape)\n",
        "\n",
        "        x = self.W_O(att)\n",
        "        #print(\"Residual shape:\")\n",
        "        #print(x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        pass\n",
        "\n",
        "    def reset_weights(self):\n",
        "\n",
        "        nn.init.uniform_(self.query_weights, a=0, b=2 * torch.pi)\n",
        "        nn.init.uniform_(self.key_weights, a=0, b=2 * torch.pi)\n",
        "        nn.init.uniform_(self.value_weights, a=0, b=2 * torch.pi)\n",
        "\n",
        "    def draw_circuit(self):\n",
        "        sample_input = torch.randn((self.cfg.d_model,))\n",
        "\n",
        "        query_drawer = qml.draw(self.query_node)\n",
        "        query_diagram = query_drawer(sample_input, self.query_weights, self.qkv_depth[0])\n",
        "\n",
        "        key_drawer = qml.draw(self.key_node)\n",
        "        key_diagram = key_drawer(sample_input, self.key_weights, self.qkv_depth[1])\n",
        "\n",
        "        value_drawer = qml.draw(self.value_node)\n",
        "        value_diagram = value_drawer(sample_input, self.value_weights, self.qkv_depth[2])\n",
        "\n",
        "\n",
        "        print(\"Query circuit:\")\n",
        "        print(query_diagram)\n",
        "        print(\"Key circuit:\")\n",
        "        print(key_diagram)\n",
        "        print(\"Value circuit:\")\n",
        "        print(value_diagram)\n",
        "\n",
        "\n",
        "    def draw_circuit_mpl(self):\n",
        "        # Generate a sample input and weights for visualization\n",
        "        sample_input = torch.randn((self.n_qubits,))\n",
        "\n",
        "        # Use qml.draw_mpl to plot the circuit\n",
        "        qml.draw_mpl(self.query_node)(sample_input, self.weights[0])\n",
        "        plt.title(\"Quantum Circuit\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "D3-CU0x8gnI_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.ln1 = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "        self.ln2 = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "        self.attn = QAttention(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "3_yfxgvtY9m-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "yhalcljNV98n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_and_load(model, reference_model, n_unfreeze): #load mlp and ln weights and freeze them except last n blocks\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      #wte and wpe matrices\n",
        "      model.embed.wte.copy_(reference_model.state_dict()['embed.W_E'])\n",
        "      model.embed.wpe.copy_(reference_model.state_dict()['pos_embed.W_pos'])\n",
        "      model.embed.wte.requires_grad_(False)\n",
        "      model.embed.wpe.requires_grad_(False)\n",
        "\n",
        "      #unembedding matrix\n",
        "      model.unembed.unembed.weight.copy_(reference_model.state_dict()['unembed.W_U'].T) #for some strange reason pretrained W_U is transposed\n",
        "      #print(model.unembed.unembed.bias.shape, reference_model.state_dict()['unembed.b_U'].shape)\n",
        "      model.unembed.unembed.bias.copy_(reference_model.state_dict()['unembed.b_U'])\n",
        "      model.unembed.unembed.requires_grad_(False)\n",
        "\n",
        "      #final LayerNorm\n",
        "      model.ln_final.weight.copy_(reference_model.state_dict()['ln_final.w'])\n",
        "      model.ln_final.bias.copy_(reference_model.state_dict()['ln_final.b'])\n",
        "      model.ln_final.requires_grad_(False)\n",
        "\n",
        "      for (n, block) in enumerate(model.blocks):\n",
        "\n",
        "        #LayerNorms\n",
        "        block.ln1.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln1.w'])\n",
        "        block.ln1.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln1.b'])\n",
        "        block.ln2.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln2.w'])\n",
        "        block.ln2.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.ln2.b'])\n",
        "        #MLP (the pretrained weights are transposed)\n",
        "        block.mlp.w_in.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.W_in'].T)\n",
        "        block.mlp.w_in.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.b_in'])\n",
        "        block.mlp.w_out.weight.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.W_out'].T)\n",
        "        block.mlp.w_out.bias.copy_(reference_model.state_dict()['blocks.'+str(n)+'.mlp.b_out'])\n",
        "        #freeze weights\n",
        "        if n in range(model.cfg.n_layers-n_unfreeze):\n",
        "          #print(\"weights in block \"+str(n)+\" frozen\")\n",
        "          block.ln1.requires_grad_(False)\n",
        "          block.ln2.requires_grad_(False)\n",
        "          block.mlp.w_in.requires_grad_(False)\n",
        "          block.mlp.w_out.requires_grad_(False)\n"
      ],
      "metadata": {
        "id": "Y41-LsvP7nCY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Transformer"
      ],
      "metadata": {
        "id": "eADST_JpPDiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGPT(nn.Module):\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = nn.LayerNorm(cfg.d_model, eps=cfg.layer_norm_eps)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "\n",
        "        residual = self.embed(tokens)\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "        logits = self.unembed(self.ln_final(residual))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "bs8rBEGwvsCj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt = QGPT(cfg)"
      ],
      "metadata": {
        "id": "TdMTjOwV3-XA"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = sum(p.numel() for p in qgpt.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrtDfXfvqub5",
        "outputId": "76a0ac81-8239-4243-f467-deef0fda7481"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 134841121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_and_load(qgpt, reference_gpt2, 2)"
      ],
      "metadata": {
        "id": "CUg4ygYkebLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = sum(p.numel() for p in qgpt.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCrbQ-fDqzmo",
        "outputId": "6c035e90-d352-4214-a8fa-a2dbc3999ce7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 9553104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt(text_tokens).shape"
      ],
      "metadata": {
        "id": "aR2DR25Lo5qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250e8646-2205-45cc-fdee-22b19b515f72"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 14, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampler"
      ],
      "metadata": {
        "id": "brMHfnxvWEUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampler:\n",
        "\n",
        "    def __init__(self, cfg, model, tokenizer):\n",
        "\n",
        "      self.cfg = cfg\n",
        "      self.model = model\n",
        "      self.tokenizer = tokenizer\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, tokens, n_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(n_new_tokens):\n",
        "            #crop sequence at context size if required\n",
        "            cropped_tokens = tokens if tokens.size(1) <= self.cfg.n_ctx else tokens[:, -self.cfg.n_ctx:]\n",
        "            #forward the model\n",
        "            logits = self.model(cropped_tokens)\n",
        "            #get the last logit and scale it by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            #apply top k\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            #softmax\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            #sample nect tokens\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            #add the sample token to the sequence\n",
        "            tokens = torch.cat((tokens, next_token), dim=1)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def sample(self, tokens, n_new_tokens, temperature=1, top_k=None):\n",
        "\n",
        "        sentences = []\n",
        "        token_sequences = self.generate(tokens, n_new_tokens, temperature, top_k)\n",
        "        for sequence in token_sequences:\n",
        "            sentence = self.tokenizer.decode(sequence)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "        return sentences"
      ],
      "metadata": {
        "id": "IUYV21O3WHTc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "F5md_-gpd-vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt_sampler = Sampler(cfg, qgpt, tokenizer)"
      ],
      "metadata": {
        "id": "ijByXuB3eAoG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt_sampler.sample(text_tokens, 10, temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaYNeJDyeXva",
        "outputId": "9ba8bed3-65dd-404e-c7f5-7573a9ed4b90"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|endoftext|>Jingle bells, jingle bells, jingle all the way, the the the the the the the the the',\n",
              " '<|endoftext|>Today I was walking home, when suddenly<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n, the the the the the the the the']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_sampler = Sampler(cfg, reference_gpt2, tokenizer)"
      ],
      "metadata": {
        "id": "xsyRLOs3j0ur"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qgpt_sampler.sample(text_tokens, 10, temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-I7sVqOkJb8",
        "outputId": "979d4587-1181-479c-cf9c-6868e06d30b0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|endoftext|>Jingle bells, jingle bells, jingle all the way, the the the.\\n\\n\\n\\n\\n',\n",
              " '<|endoftext|>Today I was walking home, when suddenly<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n, the the the the the, the']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wp9h62wNqPgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}